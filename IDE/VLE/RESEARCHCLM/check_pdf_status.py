import json
from pathlib import Path
from collections import defaultdict

# Paths
index_path = Path("r:/IDE/VLE/RESEARCHCLM/artifacts/simple_store/index.json")
case_study_dir = Path("r:/IDE/VLE/RESEARCHCLM/case_study")
other_sources_dir = Path("r:/IDE/VLE/RESEARCHCLM/other_sources")

print("=" * 70)
print("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ ÙˆØ¶Ø¹ÛŒØª PDFÙ‡Ø§")
print("=" * 70)

# Count total PDFs
case_study_pdfs = list(case_study_dir.glob("*.pdf")) if case_study_dir.exists() else []
other_sources_pdfs = list(other_sources_dir.glob("*.pdf")) if other_sources_dir.exists() else []

total_pdfs = len(case_study_pdfs) + len(other_sources_pdfs)

print(f"\nğŸ“ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ PDFâ€ŒÙ‡Ø§ Ø¯Ø± ÙÙˆÙ„Ø¯Ø±Ù‡Ø§:")
print(f"   ğŸ—‚ï¸  Case Study: {len(case_study_pdfs)} ÙØ§ÛŒÙ„")
print(f"   ğŸ—‚ï¸  Other Sources: {len(other_sources_pdfs)} ÙØ§ÛŒÙ„")
print(f"   ğŸ“Š Ù…Ø¬Ù…ÙˆØ¹: {total_pdfs} ÙØ§ÛŒÙ„ PDF")

# Read index to see processed files
if index_path.exists():
    with open(index_path, 'r', encoding='utf-8') as f:
        index_data = json.load(f)
    
    file_stats = index_data.get('file_stats', [])
    
    print(f"\nâœ… PDFâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ (Ú†Ø§Ù†Ú© Ø´Ø¯Ù‡):")
    print(f"   ğŸ“„ ØªØ¹Ø¯Ø§Ø¯: {len(file_stats)} ÙØ§ÛŒÙ„")
    
    # Group by folder
    by_folder = defaultdict(list)
    total_chunks = 0
    
    for file_info in file_stats:
        file_name = Path(file_info['file']).name
        folder = file_info.get('folder', 'unknown')
        chunks = file_info.get('chunks', 0)
        total_chunks += chunks
        
        by_folder[folder].append({
            'name': file_name,
            'chunks': chunks
        })
    
    for folder, files in by_folder.items():
        folder_name = "Case Study" if "case_study" in folder else "Other Sources"
        total_folder_chunks = sum(f['chunks'] for f in files)
        print(f"\n   ğŸ“‚ {folder_name}:")
        print(f"      âœ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡: {len(files)}")
        print(f"      âœ“ Ù…Ø¬Ù…ÙˆØ¹ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§: {total_folder_chunks:,}")
    
    print(f"\n   ğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ Ú©Ù„ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§: {total_chunks:,}")
    
    # Calculate percentage
    processed_count = len(file_stats)
    if total_pdfs > 0:
        percentage = (processed_count / total_pdfs) * 100
        print(f"\nğŸ“ˆ Ø¯Ø±ØµØ¯ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡: {percentage:.1f}% ({processed_count}/{total_pdfs})")
    
    # Show unprocessed
    unprocessed = total_pdfs - processed_count
    if unprocessed > 0:
        print(f"\nâš ï¸  PDFâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø´Ø¯Ù‡: {unprocessed} ÙØ§ÛŒÙ„")
        print(f"   ğŸ’¡ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ù…Ù‡: python scripts/desk_ingest.py --recreate --store-backend simple --no-embed --loader pdfplumber --split-mode sentence --sentences-per-chunk 1")
    else:
        print(f"\nğŸ‰ Ù‡Ù…Ù‡ PDFâ€ŒÙ‡Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯! âœ…")
    
    # Show sample files
    if file_stats:
        print(f"\nğŸ“‹ Ù†Ù…ÙˆÙ†Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡ (5 ØªØ§ Ø§ÙˆÙ„):")
        for i, file_info in enumerate(file_stats[:5], 1):
            file_name = Path(file_info['file']).name
            chunks = file_info.get('chunks', 0)
            print(f"   {i}. {file_name}: {chunks} Ú†Ø§Ù†Ú©")
        
        if len(file_stats) > 5:
            print(f"   ... Ùˆ {len(file_stats) - 5} ÙØ§ÛŒÙ„ Ø¯ÛŒÚ¯Ø±")

else:
    print(f"\nâŒ ÙØ§ÛŒÙ„ index.json ÛŒØ§ÙØª Ù†Ø´Ø¯!")
    print(f"   Ù‡ÛŒÚ† PDF Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")

print("\n" + "=" * 70)

