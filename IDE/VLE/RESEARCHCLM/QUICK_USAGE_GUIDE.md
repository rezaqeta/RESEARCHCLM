# Quick Usage Guide - PDF Embedding System
# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø³Ø±ÛŒØ¹ - Ø³ÛŒØ³ØªÙ… Embedding Ø§Ø³Ù†Ø§Ø¯ PDF

---

## ğŸ¯ What Was Extracted | Ú†Ù‡ Ú†ÛŒØ²ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯

âœ… **42 chunks** of content related to **policymakers** using **behavioral science**  
âœ… **42 Ú†Ø§Ù†Ú©** Ù…Ø­ØªÙˆØ§ Ø¯Ø±Ø¨Ø§Ø±Ù‡ **Ø³ÛŒØ§Ø³Øªâ€ŒÚ¯Ø°Ø§Ø±Ø§Ù†** Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Ø¹Ù„ÙˆÙ… Ø±ÙØªØ§Ø±ÛŒ**

### Source Document | Ø³Ù†Ø¯ Ù…Ø±Ø¬Ø¹:
ğŸ“„ `2019-BIT-Rare-Behavior-Change-for-Nature-digital (1).pdf`

### Key Topics | Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ:
- Policy maker interventions | Ù…Ø¯Ø§Ø®Ù„Ø§Øª Ø³ÛŒØ§Ø³Øªâ€ŒÚ¯Ø°Ø§Ø±Ø§Ù†
- Behavioral science techniques | ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¹Ù„ÙˆÙ… Ø±ÙØªØ§Ø±ÛŒ
- Nudge theory | Ù†Ø¸Ø±ÛŒÙ‡ Ù†Ø§Ø¬
- Evidence-based policy | Ø³ÛŒØ§Ø³Øª Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø´ÙˆØ§Ù‡Ø¯
- Implementation strategies | Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒÛŒ
- Conservation behavior change | ØªØºÛŒÛŒØ± Ø±ÙØªØ§Ø± Ø­ÙØ§Ø¸ØªÛŒ

---

## ğŸ“ Output Files | ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ

### 1. EMBEDDING_RESULTS_WITH_LINKS.md
**Most Important File! | Ù…Ù‡Ù…ØªØ±ÛŒÙ† ÙØ§ÛŒÙ„!**

Contains all 42 chunks with:
- Internal navigation links | Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù†Ø§ÙˆØ¨Ø±ÛŒ Ø¯Ø§Ø®Ù„ÛŒ
- Page references (e.g., p. 7) | Ø§Ø±Ø¬Ø§Ø¹ ØµÙØ­Ø§Øª
- In-text citations (e.g., BIT, 2019, p. 7) | Ø§Ø³ØªÙ†Ø§Ø¯Ø§Øª Ø¯Ø±ÙˆÙ† Ù…ØªÙ†ÛŒ
- Keywords for each chunk | Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù‡Ø± Ø¨Ø®Ø´
- Full content | Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…Ù„

### 2. policy_behavioral_chunks.json
Structured data in JSON format for programmatic access
Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ Ø¨Ù‡ ÙØ±Ù…Øª JSON Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ

### 3. policy_behavioral_report.md
Markdown report without links (simpler version)
Ú¯Ø²Ø§Ø±Ø´ Markdown Ø¨Ø¯ÙˆÙ† Ù„ÛŒÙ†Ú© (Ù†Ø³Ø®Ù‡ Ø³Ø§Ø¯Ù‡â€ŒØªØ±)

### 4. Ø®Ù„Ø§ØµÙ‡_ÙØ§Ø±Ø³ÛŒ_Ù†ØªØ§ÛŒØ¬_Ø§Ø³ØªØ®Ø±Ø§Ø¬.md
Complete Persian summary and guide
Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„ ÙØ§Ø±Ø³ÛŒ

---

## ğŸ† Best Embedding Model | Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Embedding

### â­ RECOMMENDED | ØªÙˆØµÛŒÙ‡ Ø´Ø¯Ù‡:
```
sentence-transformers/all-mpnet-base-v2
```

**Why? | Ú†Ø±Ø§ØŸ**
- âœ… Free & Open Source | Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ù…ØªÙ†â€ŒØ¨Ø§Ø²
- âœ… High quality (768 dimensions) | Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§
- âœ… Perfect for academic/policy documents | Ø¹Ø§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù†Ø§Ø¯ ØªØ­Ù‚ÛŒÙ‚Ø§ØªÛŒ
- âœ… Works offline | Ú©Ø§Ø± Ø¢ÙÙ„Ø§ÛŒÙ†
- âœ… Fast inference | Ø§Ø³ØªÙ†ØªØ§Ø¬ Ø³Ø±ÛŒØ¹

### Alternative Models | Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†:

| Model | Dimensions | Best For | Cost |
|-------|------------|----------|------|
| OpenAI text-embedding-3-large | 3072 | High precision | $0.13/1M tokens |
| BAAI/bge-large-en-v1.5 | 1024 | Retrieval tasks | Free |
| intfloat/e5-large-v2 | 1024 | General purpose | Free |
| paraphrase-multilingual-mpnet | 768 | Multilingual | Free |

---

## ğŸš€ Quick Start | Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹

### Step 1: Install Dependencies | Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§

```bash
pip install -r requirements_embedding.txt
```

Or manually:
```bash
pip install PyPDF2 sentence-transformers chromadb numpy
```

### Step 2: Extract Content (Already Done!) | Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ (Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡!)

```bash
python extract_and_embed_pdf.py
```

âœ… This has already been run and created 42 chunks!

### Step 3: Create Embeddings | Ø§ÛŒØ¬Ø§Ø¯ Embedding

```bash
python create_embeddings_vectordb.py
```

This will:
- Create embeddings for all 42 chunks
- Build a vector database (ChromaDB)
- Enable semantic search

---

## ğŸ’¡ Example Usage | Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡

### Python Code:

```python
from sentence_transformers import SentenceTransformer
import json

# Load model
model = SentenceTransformer('all-mpnet-base-v2')

# Load chunks
with open('policy_behavioral_chunks.json', 'r', encoding='utf-8') as f:
    chunks = json.load(f)

# Create embeddings
texts = [chunk['content'] for chunk in chunks]
embeddings = model.encode(texts)

print(f"Created {len(embeddings)} embeddings")
print(f"Shape: {embeddings.shape}")  # (42, 768)

# Search example
query = "policy maker behavioral intervention design"
query_embedding = model.encode(query)

# Find most similar chunks
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity([query_embedding], embeddings)[0]
top_indices = similarities.argsort()[-5:][::-1]

print("\nTop 5 most relevant chunks:")
for idx in top_indices:
    print(f"\nChunk {chunks[idx]['chunk_id']}: {chunks[idx]['heading'][:50]}")
    print(f"Page: {chunks[idx]['page_number']}")
    print(f"Similarity: {similarities[idx]:.3f}")
```

---

## ğŸ” Sample Search Queries | Ù†Ù…ÙˆÙ†Ù‡ Ú©ÙˆØ¦Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ

Try these queries in your semantic search:

### English:
1. "How do policymakers use behavioral science?"
2. "Effective nudge interventions for government"
3. "Behavioral insights implementation challenges"
4. "Evidence-based policy design methods"
5. "Conservation behavior change strategies"

### ÙØ§Ø±Ø³ÛŒ (Persian):
1. "Ú†Ú¯ÙˆÙ†Ù‡ Ø³ÛŒØ§Ø³Øªâ€ŒÚ¯Ø°Ø§Ø±Ø§Ù† Ø§Ø² Ø¹Ù„ÙˆÙ… Ø±ÙØªØ§Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŸ"
2. "Ù…Ø¯Ø§Ø®Ù„Ø§Øª Ù†Ø§Ø¬ Ù…ÙˆØ«Ø± Ø¨Ø±Ø§ÛŒ Ø¯ÙˆÙ„Øª"
3. "Ú†Ø§Ù„Ø´â€ŒÙ‡Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ø±ÙØªØ§Ø±ÛŒ"
4. "Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø·Ø±Ø§Ø­ÛŒ Ø³ÛŒØ§Ø³Øª Ù…Ø¨ØªÙ†ÛŒ Ø¨Ø± Ø´ÙˆØ§Ù‡Ø¯"
5. "Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ ØªØºÛŒÛŒØ± Ø±ÙØªØ§Ø± Ø­ÙØ§Ø¸ØªÛŒ"

---

## ğŸ“Š Content Statistics | Ø¢Ù…Ø§Ø± Ù…Ø­ØªÙˆØ§

| Metric | Value |
|--------|-------|
| Total pages in PDF | 84 |
| Chunks extracted | 42 |
| Average chunk length | ~500 words |
| Page range | 2-82 |
| Unique keywords | 17 |
| Sections with headings | 30 |

### Keyword Distribution:

```
policy/government: 35 chunks
behavioral science: 38 chunks
intervention: 32 chunks
behavior change: 40 chunks
implementation: 25 chunks
nudge: 15 chunks
evidence: 28 chunks
practitioner: 30 chunks
```

---

## ğŸ¯ Use Cases | Ù…ÙˆØ§Ø±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡

### 1. Research | ØªØ­Ù‚ÛŒÙ‚
- Find relevant quotes with page numbers
- Compare different approaches
- Build literature review

### 2. Policy Making | Ø³ÛŒØ§Ø³Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ
- Learn behavioral intervention techniques
- Find implementation strategies
- Access case studies

### 3. Teaching | Ø¢Ù…ÙˆØ²Ø´
- Create course materials
- Find real-world examples
- Build presentations

### 4. AI/ML Projects | Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
- Build RAG (Retrieval Augmented Generation) systems
- Create chatbots with policy knowledge
- Semantic search applications

---

## ğŸ”§ Advanced Usage | Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡

### Build a Semantic Search API

```python
from chromadb import Client
from chromadb.config import Settings

# Initialize ChromaDB
client = Client(Settings(persist_directory="./chroma_db"))

# Get or create collection
collection = client.get_or_create_collection("policy_behavioral")

# Query
results = collection.query(
    query_texts=["behavioral intervention design"],
    n_results=5
)

for doc, metadata in zip(results['documents'][0], results['metadatas'][0]):
    print(f"Page {metadata['page_number']}: {doc[:200]}...")
```

### Create a RAG System

```python
from openai import OpenAI
import chromadb

def answer_with_context(question):
    # Search relevant chunks
    collection = chromadb.Client().get_collection("policy_behavioral")
    results = collection.query(query_texts=[question], n_results=3)
    
    # Combine context
    context = "\n\n".join(results['documents'][0])
    
    # Ask LLM with context
    client = OpenAI()
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a behavioral science expert."},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {question}"}
        ]
    )
    
    return response.choices[0].message.content

# Example
answer = answer_with_context("How can policymakers apply nudge theory?")
print(answer)
```

---

## ğŸ“– Navigation Tips | Ù†Ú©Ø§Øª Ù†Ø§ÙˆØ¨Ø±ÛŒ

### In EMBEDDING_RESULTS_WITH_LINKS.md:

1. **Use the Table of Contents** at the top to jump directly to any chunk
2. **Each chunk has**:
   - Heading and subheading
   - Page reference (ğŸ“„)
   - Citation (ğŸ“š)
   - Keywords (ğŸ”‘)
   - Full content
   - Back to top link (â¬†ï¸)

3. **Search within the file** (Ctrl+F / Cmd+F) for specific terms

---

## âœ… What's Been Done | Ø¢Ù†Ú†Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡

- [x] Extract text from PDF with page numbers
- [x] Identify policy maker & behavioral science content
- [x] Create 42 chunks with proper references
- [x] Generate in-text citations (BIT, 2019, p. X)
- [x] Extract keywords for each chunk
- [x] Create markdown report with internal links
- [x] Create JSON data file
- [x] Generate Persian summary
- [x] Recommend best embedding models

---

## â­ï¸ Next Steps | Ù…Ø±Ø§Ø­Ù„ Ø¨Ø¹Ø¯ÛŒ

### To Create Embeddings:

```bash
# Install dependencies
pip install sentence-transformers torch

# Run embedding creation
python create_embeddings_vectordb.py
```

### To Build Vector Database:

```bash
# Install ChromaDB
pip install chromadb

# Run (automatically creates DB)
python create_embeddings_vectordb.py
```

### To Use in Your Application:

```python
# Load the JSON data
import json
with open('policy_behavioral_chunks.json', 'r') as f:
    chunks = json.load(f)

# Each chunk has:
# - chunk_id: unique identifier
# - heading: section heading
# - subheading: subsection
# - content: full text
# - page_number: source page
# - in_text_citation: citation format
# - keywords: relevant terms
```

---

## ğŸ“ Learn More | Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±

### About Embedding Models:
- [Sentence Transformers Documentation](https://www.sbert.net/)
- [Hugging Face Model Hub](https://huggingface.co/models)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)

### About Behavioral Science:
- Behavioural Insights Team: www.bi.team
- Rare Organization: www.rare.org

### About This Document:
- Source: BIT & Rare (2019) "Behavior Change For Nature"
- Pages: 84 total
- Focus: Policy applications of behavioral science

---

## ğŸ’¬ Support | Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ

### If you encounter issues:

1. **Dependencies not installed?**
   ```bash
   pip install -r requirements_embedding.txt
   ```

2. **Can't find files?**
   All files are in: `R:\IDE\VLE\RESEARCHCLM\`

3. **Want more chunks?**
   Edit `extract_and_embed_pdf.py` and change `min_chunks=30` to a higher number

4. **Need different keywords?**
   Edit the keywords list in `search_policy_behavioral_content()` function

---

## ğŸ“Š Summary | Ø®Ù„Ø§ØµÙ‡

âœ… **42 high-quality chunks** extracted from 84-page PDF  
âœ… **All chunks have page references** for proper citation  
âœ… **Complete markdown report** with internal navigation  
âœ… **JSON data** for programmatic access  
âœ… **Persian summary** included  
âœ… **Best embedding model** recommended: `all-mpnet-base-v2`  

### Files to Review:
1. ğŸ“„ **EMBEDDING_RESULTS_WITH_LINKS.md** â† START HERE!
2. ğŸ“„ **Ø®Ù„Ø§ØµÙ‡_ÙØ§Ø±Ø³ÛŒ_Ù†ØªØ§ÛŒØ¬_Ø§Ø³ØªØ®Ø±Ø§Ø¬.md** â† Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒâ€ŒØ²Ø¨Ø§Ù†Ø§Ù†
3. ğŸ’¾ **policy_behavioral_chunks.json** â† Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ

---

**ğŸ‰ Your PDF has been successfully processed and is ready to use!**  
**ğŸ‰ PDF Ø´Ù…Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø³Øª!**

