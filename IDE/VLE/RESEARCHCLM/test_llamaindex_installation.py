"""
Test LlamaIndex Installation
Check if chunking and embedding work properly
"""

import sys
import os

print("=" * 70)
print("ğŸ” LlamaIndex Installation Test")
print("=" * 70)

# Test 1: Import LlamaIndex
print("\n1ï¸âƒ£ Testing LlamaIndex Import...")
try:
    from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
    from llama_index.core.node_parser import SentenceSplitter
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    print("   âœ… All imports successful!")
except ImportError as e:
    print(f"   âŒ Import failed: {e}")
    sys.exit(1)

# Test 2: Create sample document
print("\n2ï¸âƒ£ Testing Document Creation...")
try:
    from llama_index.core import Document
    
    # Create test documents
    doc1 = Document(text="Behavioral interventions have shown significant impact on climate change mitigation through nudging and social norms.")
    doc2 = Document(text="Climate policy recommendations include carbon pricing, renewable energy subsidies, and energy efficiency standards.")
    doc3 = Document(text="Water conservation strategies involve behavioral change, policy interventions, and technological solutions.")
    
    documents = [doc1, doc2, doc3]
    print(f"   âœ… Created {len(documents)} test documents")
except Exception as e:
    print(f"   âŒ Document creation failed: {e}")
    sys.exit(1)

# Test 3: Text Chunking
print("\n3ï¸âƒ£ Testing Text Chunking...")
try:
    splitter = SentenceSplitter(
        chunk_size=100,
        chunk_overlap=20
    )
    
    chunks = splitter.get_nodes_from_documents(documents)
    print(f"   âœ… Created {len(chunks)} chunks from {len(documents)} documents")
    
    # Show first chunk
    if chunks:
        print(f"   ğŸ“„ Sample chunk: '{chunks[0].text[:80]}...'")
except Exception as e:
    print(f"   âŒ Chunking failed: {e}")
    sys.exit(1)

# Test 4: Local Embeddings (No API key needed)
print("\n4ï¸âƒ£ Testing Local Embeddings...")
try:
    print("   ğŸ”„ Loading embedding model (this may take a moment)...")
    
    embed_model = HuggingFaceEmbedding(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    
    Settings.embed_model = embed_model
    print("   âœ… Embedding model loaded successfully!")
    
    # Test embedding a sample text
    test_text = "This is a test sentence for embedding."
    embedding = embed_model.get_text_embedding(test_text)
    
    print(f"   âœ… Created embedding with dimension: {len(embedding)}")
    print(f"   ğŸ“Š Sample values: [{embedding[0]:.4f}, {embedding[1]:.4f}, {embedding[2]:.4f}, ...]")
except Exception as e:
    print(f"   âŒ Embedding failed: {e}")
    sys.exit(1)

# Test 5: Create Vector Index
print("\n5ï¸âƒ£ Testing Vector Index Creation...")
try:
    print("   ğŸ”„ Building vector index...")
    
    index = VectorStoreIndex(chunks, show_progress=False)
    print("   âœ… Vector index created successfully!")
except Exception as e:
    print(f"   âŒ Index creation failed: {e}")
    sys.exit(1)

# Test 6: Semantic Search
print("\n6ï¸âƒ£ Testing Semantic Search...")
try:
    query = "behavioral interventions for climate"
    
    retriever = index.as_retriever(similarity_top_k=2)
    results = retriever.retrieve(query)
    
    print(f"   âœ… Search successful! Found {len(results)} results")
    
    for i, result in enumerate(results, 1):
        print(f"\n   Result {i} (Score: {result.score:.3f}):")
        print(f"   Text: {result.text[:80]}...")
except Exception as e:
    print(f"   âŒ Search failed: {e}")
    sys.exit(1)

# Test 7: PDF Loading (if PDFs exist)
print("\n7ï¸âƒ£ Testing PDF Loading...")
try:
    pdf_dir = "./sources"
    
    if os.path.exists(pdf_dir):
        pdf_docs = SimpleDirectoryReader(
            pdf_dir,
            required_exts=[".pdf"],
            recursive=True
        ).load_data()
        
        if pdf_docs:
            print(f"   âœ… Loaded {len(pdf_docs)} PDF documents")
            
            # Chunk PDFs
            pdf_chunks = splitter.get_nodes_from_documents(pdf_docs[:1])  # Test with first PDF only
            print(f"   âœ… Created {len(pdf_chunks)} chunks from first PDF")
        else:
            print(f"   âš ï¸  No PDFs found in {pdf_dir}")
    else:
        print(f"   âš ï¸  PDF directory {pdf_dir} not found")
except Exception as e:
    print(f"   âš ï¸  PDF loading: {e}")

# Final Summary
print("\n" + "=" * 70)
print("ğŸ“Š INSTALLATION TEST SUMMARY")
print("=" * 70)

print("\nâœ… All Core Features Working:")
print("   âœ“ LlamaIndex imports")
print("   âœ“ Document creation")
print("   âœ“ Text chunking (splitting)")
print("   âœ“ Local embeddings (no API key)")
print("   âœ“ Vector index creation")
print("   âœ“ Semantic search")
print("   âœ“ PDF loading (if available)")

print("\nğŸ‰ SUCCESS! LlamaIndex is fully functional!")
print("\nğŸ“ You can now:")
print("   â€¢ Chunk documents: âœ“")
print("   â€¢ Create embeddings: âœ“")
print("   â€¢ Build search index: âœ“")
print("   â€¢ Query documents: âœ“")

print("\nğŸš€ Ready to use! Try:")
print("   python llamaindex_local_search.py")

print("\n" + "=" * 70)




